<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yunzhi Zhang</title>
    <meta name="author" content="Yunhi Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/style.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172505576-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-172505576-1');
    </script>

</head>

<body>
    <section>
        <header><h2>Yunzhi Zhang</h2></header>
        <br>

        <p> I am currently working on autonomous robots at <a href="https://covariant.ai/">covariant.ai</a>.
            I am also affiliated with
            Professor <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
            as a visiting Ph.D. student at UC Berkeley.
            <br>
            I received B.A. in computer science and pure mathematics from UC Berkeley. Starting Fall 2021,
            I will be pursuing Ph.D. in CS at Stanford University.
        </p>
        <div style="text-align:center">
            <a href="https://scholar.google.com/citations?user=BzWl0r0AAAAJ&hl=en">Scholar</a> &nbsp/&nbsp
            <a href="https://github.com/zzyunzhi">Github</a> &nbsp/&nbsp
            <a href="mailto: yunzhizhang.x@gmail.edu">Email</a> &nbsp/&nbsp
            <a href="https://www.linkedin.com/in/yunzhi-zhang-162a49149/">LinkedIn</a>
        </div>
        <br>
        <br>
    </section>

    <section>
        <header><b>Research</b></header>

        <div>
            <table>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:top">
                        <div class="paper-img"><img src='img/MazeA-v0-goal-dist.gif' alt="vds"></div>
                    </td>
                    <td style="padding:5px;width:75%;vertical-align:middle">
                        <b>Automatic Curriculum Learning through Value Disagreement</b>
                        <br>
                        Yunzhi Zhang, Pieter Abbeel, Lerrel Pinto
                        <br>
                        <a href="https://arxiv.org/abs/2006.09641">[arXiv]</a>
                        <a href="https://sites.google.com/berkeley.edu/vds/">[Webpage]</a>
                        <a href="https://github.com/zzyunzhi/vds">[Code]</a>
                        <br>
                        <em>ICML 2020 workshop on Learning in Artificial Open Worlds. </em>
                        <br>

                        <p>
                            In goal-conditioned RL, we introduce a goal sampling strategy which prioritizes goals associated with
                            high epistemic uncertainty of a learned Q-value function.
                            This simple technique is able to provide a strong learning signal even
                            in sparse reward environments.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:top">
                        <div class="paper-img"><img src='img/metrpo-stack-asynch.gif' alt="asynch-mb"></div>
                    </td>
                    <td style="padding:5px;width:75%;vertical-align:middle">
                        <b>Asynchronous Methods for Model-based Reinforcement Learning</b>
                        <br>
                        Yunzhi Zhang*, Ignasi Clavera*, Boren Tsai, Pieter Abbeel
                        <br>
                        <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.12453&sa=D&sntz=1&usg=AFQjCNGx_4WoLOPWpGVeqk4jBE3l2LbE5A">[arXiv]</a>
                        <a href="https://sites.google.com/view/asynch-mb-rl/">[Webpage]</a>
                        <a href="https://github.com/zzyunzhi/asynch-mb">[Code]</a>
                        <br>
                        <em>CoRL 2019 (<b>Spotlight</b>). NeurIPS 2019 workshop on Deep Reinforcement Learning.</em>
                        <br>

                        <p>
                            We propose a general framework for model-based reinforcement learning methods
                            with asynchronous data-collection and model training. We showcased its wall-clock-time-wise efficiency
                            on several complex robot manipulation tasks.
                        </p>
                    </td>
                </tr>
            </table>
        </div>
        <br>
    </section>

    <section>
        <header><b>Teaching</b></header>
        <ul>
            <li>Teaching Assistant, <a href="https://cs170.org">CS170</a>: <i>Efficient Algorithms and Intractable Problems</i>, Fall 2019</li>
            <li>Teaching Assistant, <a href="http://www.eecs70.org/">CS70</a>: <i>Discrete Mathematics and Probability Theory</i>, Spring 2018</li>
        </ul>
        <br>
    </section>

    <section>
        <header><b>Honors and Awards</b></header>
        <ul>
            <li>Honorable Mentions, <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/">CRA Outstanding Undergraduate Researchers</a>, 2020</li>
            <li><a href="https://www2.eecs.berkeley.edu/Students/Awards/9/?_ga=2.144009461.1128390960.1578951378-1529009263.1578951378">Arthur M. Hopkin Award</a>, 2018-2019</li>
        </ul>
        <br>
    </section>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                Design from <a href="https://jonbarron.info/">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

</body>

</html>
