<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yunzhi Zhang</title>
    <meta name="author" content="Yunhi Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/style.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172505576-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-172505576-1');
    </script>

</head>

<body style="width:680px">
    <div style="border:none; margin-top:20%"></div>
    <h1>Yunzhi Zhang</h1>

    <section>
        <p> I am currently working on autonomous robotics at <a href="https://covariant.ai/">Covariant AI</a>.
            I received B.A. in computer science and pure mathematics from UC Berkeley (2017-2020), where
            I am grateful to have Professor <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> as research advisor.
            Starting Fall 2021, I will be pursuing my Ph.D. in CS at Stanford University.
        </p>
        <div style="text-align:center">
        <a href="mailto: yunzhi@berkeley.edu">Email</a> &nbsp/&nbsp
        <a href="https://github.com/zzyunzhi">Github</a>&nbsp/&nbsp
        <a href="https://www.linkedin.com/in/yunzhi-zhang-162a49149/">LinkedIn</a>
    </div>
        <br>
        <br>
    </section>

    <section>
        <header><b>Research</b></header>

        <div style="width:100%; border:0">
            <table>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:top">
                        <div class="paper-img"><img src='img/MazeA-v0-goal-dist.gif' alt="vds"></div>
                    </td>
                    <td style="padding:5px;width:75%;vertical-align:middle">
                        <b>Automatic Curriculum Learning through Value Disagreement</b>
                        <br>
                        Yunzhi Zhang*, Pieter Abbeel, Lerrel Pinto
                        <br>
                        <a href="https://arxiv.org/abs/2006.09641">[arXiv]</a>
                        <a href="https://sites.google.com/berkeley.edu/vds/">[Webpage]</a>
                        <a href="https://github.com/zzyunzhi/vds">[Code]</a>
                        <br>
                        <em>Preprint.</em>
                        <br>

                        <p>
                            For goal-conditioned RL, we introduce a goal sampling strategy that prioritizes goals that maximize
                            the epistemic uncertainty of a learned Q-value function.
                            This simple technique fascilitates learning by providing a strong learning signal even with sparse-reward.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:top">
                        <div class="paper-img"><img src='img/metrpo-stack-asynch.gif' alt="asynch-mb"></div>
                    </td>
                    <td style="padding:5px;width:75%;vertical-align:middle">
                        <b>Asynchronous Methods for Model-based Reinforcement Learning</b>
                        <br>
                        Yunzhi Zhang*, Ignasi Clavera*, Boren Tsai, Pieter Abbeel
                        <br>
                        <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1910.12453&sa=D&sntz=1&usg=AFQjCNGx_4WoLOPWpGVeqk4jBE3l2LbE5A">[arXiv]</a>
                        <a href="https://sites.google.com/view/asynch-mb-rl/">[Webpage]</a>
                        <a href="https://github.com/zzyunzhi/asynch-mb">[Code]</a>
                        <br>
                        <em>CoRL, 2019 (<b>Spotlight</b>). NeurIPS Deep RL Workshop, 2019.</em>
                        <br>

                        <p>We propose an asynchronous framework for general model-based reinforcement learning methods which parallelizes data-collection and model training.
                            Apart from improving wall-clock-time efficiency, the asynchronous setting encourages exploration and reduces policy overfitting.</p>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <br>
    </section>

    <section>
        <header><b>Teaching</b></header>
            <ul>
                <li>Teaching Assistant, <a href="https://cs170.org">CS170</a>: <i>Efficient Algorithms and Intractable Problems</i>, Fall 2019</li>
                <li>Teaching Assistant, <a href="http://www.eecs70.org/">CS70</a>: <i>Discrete Mathematics and Probability Theory</i>, Spring 2018</li>
            </ul>
            <br>
            <br>
    </section>

    <section>
        <header><b>Honors and Awards</b></header>
        <ul>
            <li>Honorable Mentions, <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/">CRA Outstanding Undergraduate Researchers</a>, 2020</li>
            <li><a href="https://www2.eecs.berkeley.edu/Students/Awards/9/?_ga=2.144009461.1128390960.1578951378-1529009263.1578951378">Arthur M. Hopkin Award</a>, 2018-2019</li>
        </ul>
    </section>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

</body>

</html>
